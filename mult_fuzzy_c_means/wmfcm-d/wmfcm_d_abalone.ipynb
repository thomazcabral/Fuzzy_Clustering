{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609b96ac",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d24bcb9352fbc662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:47:35.174123Z",
     "start_time": "2024-07-23T22:47:33.507128Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614723f2",
   "metadata": {},
   "source": [
    "## Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0615bda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7968/3598536060.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"Class\"].replace({\"M\": 0, \"F\": 1, \"I\": 2}, inplace=True)\n",
      "/tmp/ipykernel_7968/3598536060.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"Class\"].replace({\"M\": 0, \"F\": 1, \"I\": 2}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.455 ,  0.365 ,  0.095 , ...,  0.101 ,  0.15  , 15.    ],\n",
       "       [ 0.35  ,  0.265 ,  0.09  , ...,  0.0485,  0.07  ,  7.    ],\n",
       "       [ 0.53  ,  0.42  ,  0.135 , ...,  0.1415,  0.21  ,  9.    ],\n",
       "       ...,\n",
       "       [ 0.6   ,  0.475 ,  0.205 , ...,  0.2875,  0.308 ,  9.    ],\n",
       "       [ 0.625 ,  0.485 ,  0.15  , ...,  0.261 ,  0.296 , 10.    ],\n",
       "       [ 0.71  ,  0.555 ,  0.195 , ...,  0.3765,  0.495 , 12.    ]],\n",
       "      shape=(4177, 8))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/workspaces/Fuzzy_Clustering/datasets/abalone.csv')\n",
    "df = df.rename(columns={'Sex': 'Class'})\n",
    "df[\"Class\"].replace({\"M\": 0, \"F\": 1, \"I\": 2}, inplace=True)\n",
    "labels = df[\"Class\"].values\n",
    "df.drop(\"Class\", axis=1, inplace=True)\n",
    "dados = df.to_numpy()\n",
    "dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e68412",
   "metadata": {},
   "source": [
    "## Método de agrupamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549c38ec38602e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCM():\n",
    "    def __init__(self, c, X, m):\n",
    "        self.c = c\n",
    "        self.n = X.shape[0]\n",
    "        self.p = X.shape[1]\n",
    "        self.m = m\n",
    "\n",
    "        ##\n",
    "        self.global_var = np.var(X, axis=0).mean() # pré-calcula a variância global dos dados para regularização\n",
    "\n",
    "    def initialize_u(self):\n",
    "        u_flat = np.random.dirichlet(alpha=np.ones(self.c * self.p), size=self.n)\n",
    "        return u_flat.reshape(self.n, self.c, self.p)\n",
    "    \n",
    "    def initialize_lambda(self):\n",
    "        return np.ones((self.c, self.p))\n",
    "    \n",
    "    def find_centroides(self, X, U):\n",
    "        u_m = U ** self.m\n",
    "        numerador = np.sum(u_m * X[:, np.newaxis, :], axis=0)\n",
    "        denominador = np.sum(u_m, axis=0)\n",
    "        denominador = np.fmax(denominador, np.finfo(np.float64).eps)\n",
    "        return numerador / denominador\n",
    "    \n",
    "    def get_distances(self, X, V):\n",
    "        return (X[:, np.newaxis, :] - V[np.newaxis, :, :]) ** 2\n",
    "\n",
    "    def update_u(self, D, Lambda):\n",
    "        power = 1.0 / (self.m - 1)\n",
    "        eps = np.finfo(np.float64).eps\n",
    "        \n",
    "        weighted_dist = D * Lambda\n",
    "        weighted_dist = np.fmax(weighted_dist, eps) \n",
    "        \n",
    "        term = weighted_dist ** (-power)\n",
    "        \n",
    "        denominator = np.sum(term, axis=(1, 2), keepdims=True)\n",
    "        denominator = np.fmax(denominator, eps)\n",
    "        \n",
    "        return term / denominator\n",
    "    \n",
    "    def update_lambda(self, D, U):\n",
    "        eps = np.finfo(np.float64).eps\n",
    "        \n",
    "        term_k = (U ** self.m) * np.fmax(D, eps)\n",
    "        S_ij = np.sum(term_k, axis=0) \n",
    "        \n",
    "        # Regularização Aditiva\n",
    "        ##\n",
    "        regularization_factor = 0.01 * self.global_var\n",
    "        S_ij = S_ij + regularization_factor \n",
    "        \n",
    "        prod_S = np.prod(S_ij, axis=1, keepdims=True)\n",
    "        numerator = prod_S ** (1.0 / self.p)\n",
    "        \n",
    "        Lambda = numerator / S_ij\n",
    "        \n",
    "        return Lambda\n",
    "\n",
    "    ##\n",
    "    def calculate_objective_function(self, D, U, Lambda):\n",
    "        term = (U ** self.m) * D\n",
    "        sum_k = np.sum(term, axis=0)\n",
    "        return np.sum(Lambda * sum_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a5722d",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024b9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcm_run(dados, num_clusters, m=2, max_iter=1000, epsilon=1e-6):\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(dados)\n",
    "    \n",
    "    mfcm = MFCM(c=num_clusters, X=X_scaled, m=m)\n",
    "    \n",
    "    indices = np.random.choice(X_scaled.shape[0], num_clusters, replace=False)\n",
    "    centroids = X_scaled[indices]\n",
    "    \n",
    "    Lambda = np.ones((num_clusters, X_scaled.shape[1]))\n",
    "    \n",
    "    D = mfcm.get_distances(X_scaled, centroids)\n",
    "    U = mfcm.update_u(D, Lambda)\n",
    "    \n",
    "    WARM_UP_ITERS = 20 \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        U_old = U.copy()\n",
    "        \n",
    "        centroids = mfcm.find_centroides(X_scaled, U)\n",
    "        D = mfcm.get_distances(X_scaled, centroids)\n",
    "        \n",
    "        ##\n",
    "        if i >= WARM_UP_ITERS:\n",
    "            Lambda = mfcm.update_lambda(D, U) \n",
    "        \n",
    "        U = mfcm.update_u(D, Lambda)\n",
    "        \n",
    "        if np.linalg.norm(U - U_old) < epsilon:\n",
    "            break\n",
    "        \n",
    "    J_final = mfcm.calculate_objective_function(D, U, Lambda)\n",
    "    Delta = np.sum(U, axis=2)\n",
    "    \n",
    "    return centroids, U, Delta, Lambda, J_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037a9b7",
   "metadata": {},
   "source": [
    "## Simulação de Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1dc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_objective_function(self, D, U, Lambda):\n",
    "        term = (U ** self.m) * D\n",
    "        sum_k = np.sum(term, axis=0)\n",
    "        weighted_sum = Lambda * sum_k\n",
    "        J = np.sum(weighted_sum)\n",
    "        return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0e422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_experiment(dados, labels, num_clusters=3, num_trials=100, restarts=50):\n",
    "    results_ari = []\n",
    "    results_ami = []\n",
    "    \n",
    "    for t in range(num_trials):\n",
    "        best_J = np.inf\n",
    "        best_pred = None\n",
    "        \n",
    "        for r in range(restarts):\n",
    "            try:\n",
    "                centroids, U, Delta, Lambda, J = mfcm_run(dados, num_clusters, m=2)\n",
    "                \n",
    "                if J > 1e-3 and J < best_J:\n",
    "                    best_J = J\n",
    "                    best_pred = np.argmax(Delta, axis=1)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if best_pred is not None:\n",
    "            ari = adjusted_rand_score(labels, best_pred)\n",
    "            ami = adjusted_mutual_info_score(labels, best_pred)\n",
    "            \n",
    "            results_ari.append(ari)\n",
    "            results_ami.append(ami)\n",
    "            \n",
    "            print(f\"Trial {t+1}: J={best_J:.4f} | ARI={ari:.4f} | AMI={ami:.4f}\")\n",
    "        else:\n",
    "            print(f\"Trial {t+1}: Falha (Singularidade em todas as tentativas).\")\n",
    "\n",
    "    print(f\"Mean ARI: {np.mean(results_ari):.4f} +/- {np.std(results_ari):.4f}\")\n",
    "    print(f\"Mean AMI: {np.mean(results_ami):.4f} +/- {np.std(results_ami):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f81e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: J=0.0469 | ARI=0.0284 | AMI=0.0385\n",
      "Trial 2: J=0.0469 | ARI=0.0446 | AMI=0.0571\n",
      "Trial 3: J=0.0468 | ARI=0.0419 | AMI=0.0565\n",
      "Trial 4: J=0.0479 | ARI=0.0099 | AMI=0.0249\n",
      "Trial 5: J=0.0464 | ARI=0.0272 | AMI=0.0503\n",
      "Trial 6: J=0.0469 | ARI=0.0183 | AMI=0.0452\n",
      "Trial 7: J=0.0467 | ARI=0.0105 | AMI=0.0384\n",
      "Trial 8: J=0.0463 | ARI=0.0026 | AMI=0.0045\n",
      "Trial 9: J=0.0468 | ARI=0.0097 | AMI=0.0293\n",
      "Trial 10: J=0.0463 | ARI=0.0145 | AMI=0.0260\n",
      "Trial 11: J=0.0464 | ARI=0.0176 | AMI=0.0219\n",
      "Trial 12: J=0.0469 | ARI=0.0313 | AMI=0.0403\n",
      "Trial 13: J=0.0468 | ARI=0.0259 | AMI=0.0331\n",
      "Trial 14: J=0.0469 | ARI=0.0159 | AMI=0.0482\n",
      "Trial 15: J=0.0470 | ARI=0.0231 | AMI=0.0636\n",
      "Trial 16: J=0.0468 | ARI=0.0128 | AMI=0.0264\n",
      "Trial 17: J=0.0472 | ARI=0.0445 | AMI=0.0614\n",
      "Trial 18: J=0.0469 | ARI=0.0180 | AMI=0.0304\n",
      "Trial 19: J=0.0461 | ARI=0.0228 | AMI=0.0302\n",
      "Trial 20: J=0.0461 | ARI=0.0220 | AMI=0.0458\n",
      "Trial 21: J=0.0467 | ARI=0.0391 | AMI=0.0475\n",
      "Trial 22: J=0.0464 | ARI=0.0100 | AMI=0.0245\n",
      "Trial 23: J=0.0467 | ARI=0.0067 | AMI=0.0204\n",
      "Trial 24: J=0.0467 | ARI=0.0146 | AMI=0.0370\n",
      "Trial 25: J=0.0467 | ARI=0.0057 | AMI=0.0138\n",
      "Trial 26: J=0.0466 | ARI=0.0244 | AMI=0.0491\n",
      "Trial 27: J=0.0463 | ARI=0.0046 | AMI=0.0077\n",
      "Trial 28: J=0.0473 | ARI=0.0183 | AMI=0.0273\n",
      "Trial 29: J=0.0466 | ARI=0.0157 | AMI=0.0259\n",
      "Trial 30: J=0.0468 | ARI=0.0184 | AMI=0.0528\n",
      "Trial 31: J=0.0471 | ARI=0.0371 | AMI=0.0482\n",
      "Trial 32: J=0.0477 | ARI=0.0142 | AMI=0.0226\n",
      "Trial 33: J=0.0471 | ARI=0.0354 | AMI=0.0484\n",
      "Trial 34: J=0.0461 | ARI=0.0412 | AMI=0.0566\n",
      "Trial 35: J=0.0471 | ARI=0.0134 | AMI=0.0325\n",
      "Trial 36: J=0.0470 | ARI=0.0373 | AMI=0.0506\n",
      "Trial 37: J=0.0469 | ARI=0.0268 | AMI=0.0344\n",
      "Trial 38: J=0.0464 | ARI=0.0184 | AMI=0.0392\n",
      "Trial 39: J=0.0474 | ARI=-0.0001 | AMI=0.0015\n",
      "Trial 40: J=0.0468 | ARI=0.0250 | AMI=0.0367\n",
      "Trial 41: J=0.0471 | ARI=0.0392 | AMI=0.0657\n",
      "Trial 42: J=0.0468 | ARI=0.0217 | AMI=0.0323\n",
      "Trial 43: J=0.0469 | ARI=0.0312 | AMI=0.0563\n",
      "Trial 44: J=0.0464 | ARI=0.0288 | AMI=0.0446\n",
      "Trial 45: J=0.0461 | ARI=0.0158 | AMI=0.0257\n",
      "Trial 46: J=0.0462 | ARI=0.0101 | AMI=0.0226\n",
      "Trial 47: J=0.0472 | ARI=0.0293 | AMI=0.0833\n",
      "Trial 48: J=0.0466 | ARI=0.0072 | AMI=0.0164\n",
      "Trial 49: J=0.0469 | ARI=0.0251 | AMI=0.0742\n",
      "Trial 50: J=0.0473 | ARI=0.0186 | AMI=0.0691\n",
      "Trial 51: J=0.0472 | ARI=0.0397 | AMI=0.0518\n",
      "Trial 52: J=0.0469 | ARI=0.0079 | AMI=0.0192\n",
      "Trial 53: J=0.0464 | ARI=0.0188 | AMI=0.0240\n",
      "Trial 54: J=0.0468 | ARI=0.0330 | AMI=0.0507\n",
      "Trial 55: J=0.0471 | ARI=0.0408 | AMI=0.0579\n",
      "Trial 56: J=0.0464 | ARI=0.0123 | AMI=0.0351\n",
      "Trial 57: J=0.0470 | ARI=0.0126 | AMI=0.0205\n",
      "Trial 58: J=0.0466 | ARI=0.0068 | AMI=0.0187\n",
      "Trial 59: J=0.0472 | ARI=0.0200 | AMI=0.0310\n",
      "Trial 60: J=0.0474 | ARI=0.0075 | AMI=0.0126\n",
      "Trial 61: J=0.0471 | ARI=0.0318 | AMI=0.0425\n",
      "Trial 62: J=0.0471 | ARI=0.0392 | AMI=0.0493\n",
      "Trial 63: J=0.0473 | ARI=0.0346 | AMI=0.0455\n",
      "Trial 64: J=0.0473 | ARI=0.0114 | AMI=0.0241\n",
      "Trial 65: J=0.0470 | ARI=0.0074 | AMI=0.0221\n",
      "Trial 66: J=0.0468 | ARI=0.0144 | AMI=0.0425\n",
      "Trial 67: J=0.0469 | ARI=0.0100 | AMI=0.0242\n",
      "Trial 68: J=0.0473 | ARI=0.0185 | AMI=0.0508\n",
      "Trial 69: J=0.0472 | ARI=0.0066 | AMI=0.0202\n",
      "Trial 70: J=0.0475 | ARI=0.0157 | AMI=0.0397\n",
      "Trial 71: J=0.0472 | ARI=0.0162 | AMI=0.0358\n",
      "Trial 72: J=0.0470 | ARI=0.0113 | AMI=0.0218\n",
      "Trial 73: J=0.0468 | ARI=0.0103 | AMI=0.0206\n",
      "Trial 74: J=0.0467 | ARI=0.0248 | AMI=0.0522\n",
      "Trial 75: J=0.0473 | ARI=0.0101 | AMI=0.0208\n",
      "Trial 76: J=0.0474 | ARI=0.0425 | AMI=0.0553\n",
      "Trial 77: J=0.0469 | ARI=0.0136 | AMI=0.0262\n",
      "Trial 78: J=0.0472 | ARI=0.0290 | AMI=0.0450\n",
      "Trial 79: J=0.0461 | ARI=0.0056 | AMI=0.0079\n",
      "Trial 80: J=0.0472 | ARI=0.0233 | AMI=0.0265\n",
      "Trial 81: J=0.0468 | ARI=0.0123 | AMI=0.0240\n",
      "Trial 82: J=0.0478 | ARI=0.0129 | AMI=0.0279\n",
      "Trial 83: J=0.0471 | ARI=0.0163 | AMI=0.0194\n",
      "Trial 84: J=0.0461 | ARI=0.0262 | AMI=0.0449\n",
      "Trial 85: J=0.0468 | ARI=0.0324 | AMI=0.0452\n",
      "Trial 86: J=0.0462 | ARI=0.0072 | AMI=0.0103\n",
      "Trial 87: J=0.0471 | ARI=0.0392 | AMI=0.0539\n",
      "Trial 88: J=0.0467 | ARI=0.0235 | AMI=0.0383\n",
      "Trial 89: J=0.0467 | ARI=0.0150 | AMI=0.0620\n",
      "Trial 90: J=0.0466 | ARI=0.0157 | AMI=0.0242\n",
      "Trial 91: J=0.0470 | ARI=0.0082 | AMI=0.0255\n",
      "Trial 92: J=0.0462 | ARI=0.0240 | AMI=0.0322\n",
      "Trial 93: J=0.0474 | ARI=0.0087 | AMI=0.0117\n",
      "Trial 94: J=0.0467 | ARI=0.0133 | AMI=0.0423\n",
      "Trial 95: J=0.0468 | ARI=0.0136 | AMI=0.0246\n",
      "Trial 96: J=0.0463 | ARI=0.0140 | AMI=0.0303\n",
      "Trial 97: J=0.0469 | ARI=0.0170 | AMI=0.0302\n",
      "Trial 98: J=0.0469 | ARI=0.0045 | AMI=0.0165\n",
      "Trial 99: J=0.0474 | ARI=0.0142 | AMI=0.0300\n",
      "Trial 100: J=0.0462 | ARI=0.0127 | AMI=0.0220\n",
      "Mean ARI: 0.0198 +/- 0.0111\n",
      "Mean AMI: 0.0355 +/- 0.0162\n"
     ]
    }
   ],
   "source": [
    "run_final_experiment(dados, labels, num_clusters=3, num_trials=100, restarts=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
