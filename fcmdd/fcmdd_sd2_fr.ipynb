{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_configuracao(mu_list, sigma_list, tamanhos, config_id):\n",
    "    dfs = []\n",
    "    for i, (mu, sigma2, n) in enumerate(zip(mu_list, sigma_list, tamanhos)):\n",
    "        Sigma = np.diag(sigma2)\n",
    "        data = np.random.multivariate_normal(mu, Sigma, n)\n",
    "        df = pd.DataFrame(data, columns=[\"x1\", \"x2\"])\n",
    "        df[\"class\"] = i + 1\n",
    "        dfs.append(df)\n",
    "    df_config = pd.concat(dfs, ignore_index=True)\n",
    "    df_config[\"config\"] = config_id\n",
    "    return df_config\n",
    "\n",
    "np.random.seed(42)  # reprodutibilidade\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 1\n",
    "mu_1 = [[5, 0], [15, 5], [18, 14]]\n",
    "sigma2_1 = [[81, 9], [9, 100], [25, 36]]\n",
    "n1 = [200, 100, 50]\n",
    "df1 = gerar_configuracao(mu_1, sigma2_1, n1, config_id=1)\n",
    "l1 = \"Classes elípticas de tamanhos diferentes\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 2\n",
    "mu_2 = [[0, 0], [30, 0], [12, 25]]\n",
    "sigma2_2 = [[100, 100], [49, 49], [16, 16]]\n",
    "n2 = [200, 100, 50]\n",
    "df2 = gerar_configuracao(mu_2, sigma2_2, n2, config_id=2)\n",
    "l2 = \"Classes esféricas de tamanhos diferentes\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 3\n",
    "mu_3 = [[0, 0], [15, 5], [15, -5]]\n",
    "sigma2_3 = [[100, 4], [100, 4], [100, 4]]\n",
    "n3 = [100, 100, 100]\n",
    "df3 = gerar_configuracao(mu_3, sigma2_3, n3, config_id=3)\n",
    "l3 = \"Classes elípticas de tamanhos iguais\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 4\n",
    "mu_4 = [[0, 0], [15, 0], [-15, 0]]\n",
    "sigma2_4 = [[16, 16], [16, 16], [16, 16]]\n",
    "n4 = [100, 100, 100]\n",
    "df4 = gerar_configuracao(mu_4, sigma2_4, n4, config_id=4)\n",
    "l4 = \"Classes elípticas de tamanhos iguais\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 5\n",
    "mu_5 = [[5, 0], [15, 5], [10, -7], [3, 15]]\n",
    "sigma2_5 = [[81, 9], [9, 100], [49, 16], [25, 25]]\n",
    "n5 = [50, 50, 50, 50]\n",
    "df5 = gerar_configuracao(mu_5, sigma2_5, n5, config_id=5)\n",
    "l5 = \"3 classes elípticas e 1 esférica\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 6\n",
    "mu_6 = [[5, 0], [15, 5], [12, -12], [7, 17]]\n",
    "sigma2_6 = [[81, 9], [9, 100], [16, 16], [25, 25]]\n",
    "n6 = [50, 50, 50, 50]\n",
    "df6 = gerar_configuracao(mu_6, sigma2_6, n6, config_id=6)\n",
    "l6 = \"2 classes elípticas e 2 esféricas\"\n",
    "\n",
    "# -------------------------------\n",
    "# Configuração 7\n",
    "mu_7 = [[0, 0], [18, 0], [-18, 0], [0, -12]]\n",
    "sigma2_7 = [[12, 12], [20, 20], [16, 16], [81, 20]]\n",
    "n7 = [50, 50, 50, 50]\n",
    "df7 = gerar_configuracao(mu_7, sigma2_7, n7, config_id=7)\n",
    "l7 = \"1 classe elíptica e 3 esféricas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando classes em números"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crisp_to_fuzzy(y, n_clusters): # transforma o dataset em fuzzy\n",
    "    fuzzy_labels = np.zeros((len(y), n_clusters)) # cria uma array do dataset preenchida só com zeros\n",
    "    for i, label in enumerate(y):\n",
    "        fuzzy_labels[i, label] = 1 # com base na classe, o zero é substituído por um\n",
    "    return fuzzy_labels\n",
    "\n",
    "def init_membership_matrix(n, k):\n",
    "    membership_matrix = np.random.rand(n, k) # gera uma matriz inicial aleatória com valores entre 0 e 1\n",
    "    membership_matrix = membership_matrix / membership_matrix.sum(axis=1, keepdims=True) # normalização da matriz pra garantir que a soma dos graus dê um\n",
    "    return membership_matrix\n",
    "\n",
    "def init_medoids(X, c):\n",
    "    total_distances = np.sum(pairwise_distances(X), axis=1) # distância somada de cada ponto para os outros\n",
    "    \n",
    "    first_medoid_idx = np.argmin(total_distances) # ponto com menor distância total\n",
    "    medoids_indices = [first_medoid_idx]  # lista para os índices dos medoides\n",
    "    medoids = [X[first_medoid_idx]]  # lista para os medoides\n",
    "\n",
    "    for _ in range(1, c): # (1, c) pois já temos um medoide\n",
    "        max_min_dist = -np.inf # armazena a maior entre as menores distâncias\n",
    "        next_medoid_idx = -1 # armazena o índice do candidato a medoide\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if i in medoids_indices: # ignora os pontos que já são medoides\n",
    "                continue \n",
    "\n",
    "            # primeiro ele calcula a distância do ponto para cada um dos medoides\n",
    "            # depois ele seleciona a menor dessas distâncias\n",
    "            min_dist = np.min([pairwise_distances(X[i].reshape(1, -1), np.array([medoid])).flatten()[0] for medoid in medoids])\n",
    "\n",
    "            if min_dist > max_min_dist: # a maior distância entre as menores\n",
    "                max_min_dist = min_dist\n",
    "                next_medoid_idx = i\n",
    "\n",
    "        # adiciona o ponto escolhido como medoide\n",
    "        medoids_indices.append(next_medoid_idx)\n",
    "        medoids.append(X[next_medoid_idx])\n",
    "\n",
    "    return np.array(medoids)\n",
    "\n",
    "def update_membership_matrix(data, medoids, m):\n",
    "    distance_matrix = pairwise_distances(data, medoids, metric='euclidean') ** 2\n",
    "    distance_matrix = np.fmax(distance_matrix, np.finfo(np.float64).eps)  # evita que matriz_distancias seja 0, np.finfo... é o menor número maior que zero aqui\n",
    "    \n",
    "    inverse_distance_matrix = 1 / distance_matrix\n",
    "    power = 1 / (m - 1)\n",
    "    updated_membership_matrix = (inverse_distance_matrix ** power) / np.sum(inverse_distance_matrix ** power, axis=1, keepdims=True) # fórmula para atualizar os graus de pertinência\n",
    "    \n",
    "    return updated_membership_matrix\n",
    "\n",
    "def update_medoids(X, medoids, membership_matrix, m=2):\n",
    "    n, c = X.shape[0], len(medoids)\n",
    "    updated_medoids = np.copy(medoids)  # cópia dos medoides \n",
    "    chosen_medoids_indices = []  # armazena os índices dos novos medoides\n",
    "    distances = pairwise_distances(X, X)  # matriz de distância entre os pontos\n",
    "\n",
    "    for i in range(c): \n",
    "        initial_medoid_idx = np.where((X == medoids[i]).all(axis=1))[0][0]  # índice do medoide atual\n",
    "        min_weighted_distance = np.sum([(membership_matrix[k, i] ** m) * distances[k, initial_medoid_idx] for k in range(n)]) # distância ponderada do medoide atual\n",
    "        best_medoid = medoids[i] # melhor candidato a medoide\n",
    "        best_medoid_idx = initial_medoid_idx # índice do melhor candidato a medoide\n",
    "\n",
    "        for j in range(n):\n",
    "            if j in chosen_medoids_indices:  # ignorar pontos que já são medoides\n",
    "                continue\n",
    "            \n",
    "            weighted_distance = np.sum([(membership_matrix[k, i] ** m) * distances[k, j] for k in range(n)]) # distância ponderada do candidato a medoide\n",
    "\n",
    "            if weighted_distance < min_weighted_distance: # caso a distância ponderada do candidato seja menor que a do medoide atual\n",
    "                min_weighted_distance = weighted_distance # atualiza a menor distância ponderada\n",
    "                best_medoid = X[j] # atualiza o melhor candidato a medoide\n",
    "                best_medoid_idx = j # atualiza o índice do melhor candidato a medoide\n",
    "                updated_medoids[i] = best_medoid # já coloca ele como medoide\n",
    "\n",
    "        updated_medoids[i] = best_medoid # atualiza de fato os medoides\n",
    "        chosen_medoids_indices.append(best_medoid_idx) # para garantir que ele não será escolhido como medoide novamente\n",
    "\n",
    "    return updated_medoids\n",
    "\n",
    "def fcmdd(data, k, m=2, max_iter=1000000):\n",
    "    n = data.shape[0]\n",
    "    membership_matrix = init_membership_matrix(n, k)\n",
    "    medoids = init_medoids(data, k)\n",
    "    for _ in range(max_iter):\n",
    "        membership_matrix = update_membership_matrix(data, medoids, m)\n",
    "        new_medoids = update_medoids(data, medoids, membership_matrix, m)\n",
    "        if np.array_equal(medoids, new_medoids): # se os medoides não mudaram, para\n",
    "            break\n",
    "        medoids = new_medoids\n",
    "    return medoids, membership_matrix\n",
    "\n",
    "def pertinence_distance(delta_k, delta_k_linha, c):\n",
    "    # calcula a distância entre δ_k e δ_k' (matrizes de pertinência)\n",
    "    return (1/c) * np.sum((delta_k - delta_k_linha) ** 2)\n",
    "\n",
    "def fuzzy_rand_index(particao1, particao2, c):\n",
    "    n = particao1.shape[0]\n",
    "    total_sum = 0\n",
    "\n",
    "    for k in range(n):\n",
    "        for k_linha in range(k+1, n):\n",
    "            if k != k_linha:\n",
    "                # calcula a métrica para P\n",
    "                delta_k = particao1[k]\n",
    "                delta_k_prime = particao1[k_linha]\n",
    "                EP = 1 - pertinence_distance(delta_k, delta_k_prime, c)\n",
    "\n",
    "                # calcula a métrica para Q\n",
    "                delta_k_Q = particao2[k]\n",
    "                delta_k_prime_Q = particao2[k_linha]\n",
    "                EQ = 1 - pertinence_distance(delta_k_Q, delta_k_prime_Q, c)\n",
    "\n",
    "                total_sum += np.abs(EP - EQ) # soma a diferença absoluta entre EP e EQ\n",
    "\n",
    "    denominador = n * (n - 1) / 2\n",
    "    if denominador == 0:\n",
    "        raise ValueError\n",
    "\n",
    "    return 1- (total_sum / denominador)\n",
    "\n",
    "def monte_carlo_fuzzy_simulation(X, true_labels, k, m=2, num_trials=100):\n",
    "    results = []\n",
    "    for trial in range(num_trials):\n",
    "        medoids, membership_matrix = fcmdd(X, k, m)\n",
    "        predicted_labels = np.argmax(membership_matrix, axis=1)\n",
    "        rand_idx = fuzzy_rand_index(true_labels, predicted_labels, k)\n",
    "        results.append(rand_idx)\n",
    "    \n",
    "    mean_ari = np.mean(results)\n",
    "    std_ari = np.std(results)\n",
    "    return mean_ari, std_ari\n",
    "\n",
    "i = 1\n",
    "for df in [df1, df2, df3, df4, df5, df6, df7]:\n",
    "    if i == 5 or i == 6 or i == 7:\n",
    "        num_clusters = 4\n",
    "    else:\n",
    "        num_clusters = 3\n",
    "    df.drop(\"config\", axis=1, inplace=True)\n",
    "    if i == 5 or i == 6 or i == 7:\n",
    "        df[\"class\"].replace({1: 0, 2: 1, 3: 2, 4: 3}, inplace=True)\n",
    "    else: \n",
    "        df[\"class\"].replace({1: 0, 2: 1, 3: 2}, inplace=True)\n",
    "    labels = df[\"class\"].values\n",
    "    df.drop(\"class\", axis=1, inplace=True)\n",
    "    data = df.to_numpy()\n",
    "    labels = crisp_to_fuzzy(labels, num_clusters)\n",
    "    k = num_clusters\n",
    "    num_trials = 100\n",
    "    m = 2\n",
    "    mean_rand_index, std_rand_index = monte_carlo_fuzzy_simulation(data, labels, k, m, num_trials)\n",
    "\n",
    "    print(f\"Monte Carlo FCMdd Clustering Results for Config {i}\")\n",
    "    print(f\"Mean Rand Index: {mean_rand_index:.4f}\")\n",
    "    print(f\"Standard Deviation of Rand Index: {std_rand_index:.4f}\")\n",
    "    print(\"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados:\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 1  \n",
    "Mean Rand Index: 0.4858  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 2  \n",
    "Mean Rand Index: 0.5425  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 3  \n",
    "Mean Rand Index: 0.4962  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 4  \n",
    "Mean Rand Index: 0.6573  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 5  \n",
    "Mean Rand Index: 0.5711  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 6  \n",
    "Mean Rand Index: 0.4165  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "\n",
    "Monte Carlo FCMdd Clustering Results for Config 7  \n",
    "Mean Rand Index: 0.5462  \n",
    "Standard Deviation of Rand Index: 0.0000  \n",
    "\n",
    "**O código foi rodado no cluster Apuana CIn-UFPE**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
